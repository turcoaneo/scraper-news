{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-02T18:06:40.086791200Z",
     "start_time": "2025-11-02T18:06:40.064270900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported 712 lines to D:\\WORKSPACE\\Python\\scraper-news\\storage\\pre_declension_corpus.txt\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import os\n",
    "\n",
    "from service.util.path_util import PROJECT_ROOT\n",
    "from storage.duplets_dictionary import duplets, new_duplets\n",
    "\n",
    "BASE_DIR = PROJECT_ROOT\n",
    "CORPUS_PATH = os.path.join(BASE_DIR, \"storage\", \"pre_declension_corpus.txt\")\n",
    "\n",
    "\n",
    "def export_duplets_to_sp(p_duplets, output_path=CORPUS_PATH):\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for inflected, base in p_duplets:\n",
    "            f.write(f\"{inflected}\\n\")\n",
    "            f.write(f\"{base}\\n\")\n",
    "    print(f\"Exported {len(p_duplets) * 2} lines to {output_path}\")\n",
    "\n",
    "\n",
    "duplets = duplets + new_duplets\n",
    "export_duplets_to_sp(duplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#  Train SentencePiece tokenizer\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "# noinspection PyUnresolvedReferences\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=CORPUS_PATH,\n",
    "    model_prefix=\"pre-declension\",\n",
    "    vocab_size=300,\n",
    "    character_coverage=1.0,\n",
    "    model_type=\"unigram\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-02T18:06:40.203058300Z",
     "start_time": "2025-11-02T18:06:40.091800800Z"
    }
   },
   "id": "c2ea2b3bbc2ab99b"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\WORKSPACE\\Python\\scraper-news\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1964: FutureWarning: Calling T5Tokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Șucu']\n",
      "['▁Țiriac']\n",
      "['▁Ștefănescu']\n",
      "['▁Ștefănești']\n",
      "['▁Țepelin']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize for T5\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "pre_declension_model = \"pre-declension.model\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(pre_declension_model, legacy=False)\n",
    "\n",
    "print(tokenizer.tokenize(\"Șucu\"))       # Expect: ['▁Șucu']\n",
    "print(tokenizer.tokenize(\"Țiriac\"))     # Expect: ['▁Țiriac']\n",
    "print(tokenizer.tokenize(\"Ștefănescu\")) # Expect: ['▁Ștefăneșcu']\n",
    "print(tokenizer.tokenize(\"Ștefănești\")) # Expect: ['▁Ștefănești']\n",
    "print(tokenizer.tokenize(\"Țepelin\")) # Expect: ['▁Țepelin']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-02T18:06:49.588781300Z",
     "start_time": "2025-11-02T18:06:40.207062100Z"
    }
   },
   "id": "d4b0438969b74b48"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
