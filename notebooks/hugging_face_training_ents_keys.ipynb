{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "import os\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "from service.util.root_dir_util import get_project_root\n",
    "\n",
    "BASE_DIR = get_project_root()\n",
    "JSONL_PATH = os.path.join(BASE_DIR, \"storage\", \"hf\", \"gpt_20250930.jsonl\")\n",
    "\n",
    "dataset = Dataset.from_json(JSONL_PATH, encoding=\"utf-8\")\n",
    "dataset = dataset.train_test_split(test_size=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-05T07:55:04.875291600Z",
     "start_time": "2025-10-05T07:55:02.990294400Z"
    }
   },
   "id": "3997014fcc769914"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Define Label Mapping\n",
    "label2id = {\"O\": 0, \"B-ENT\": 1, \"B-KW\": 2}\n",
    "id2label = {v: k for k, v in label2id.items()}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-04T12:10:20.275725400Z",
     "start_time": "2025-10-04T12:10:20.238758700Z"
    }
   },
   "id": "a2ea117e90278500"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/153 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e36dff6678db4c6bb33f18a5fb5f2543"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/18 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "98a36c2226dd46f4ba62e57319a1e1bb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Convert Each Example to Tokens + Labels\n",
    "\n",
    "def split_words(txt):\n",
    "    return re.findall(r\"\\w+|\\S\", txt)\n",
    "\n",
    "\n",
    "def label_words(example):\n",
    "    summary_words = split_words(example[\"summary\"])\n",
    "    labels = [\"O\"] * len(summary_words)\n",
    "\n",
    "    def mark_span(phrase, label_tag):\n",
    "        phrase_words = split_words(phrase)\n",
    "        for i in range(len(summary_words) - len(phrase_words) + 1):\n",
    "            if summary_words[i:i + len(phrase_words)] == phrase_words:\n",
    "                for j in range(len(phrase_words)):\n",
    "                    labels[i + j] = label_tag\n",
    "\n",
    "    for ent in example[\"entities\"]:\n",
    "        mark_span(ent, \"B-ENT\")\n",
    "    for kw in example[\"keywords\"]:\n",
    "        mark_span(kw, \"B-KW\")\n",
    "\n",
    "    return {\"words\": summary_words, \"word_labels\": labels}\n",
    "\n",
    "\n",
    "dataset = dataset.map(label_words)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-04T12:10:20.549289800Z",
     "start_time": "2025-10-04T12:10:20.253732900Z"
    }
   },
   "id": "b1712cb1f22f3cca"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/153 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6e038a63eef348489e1cce4e733f35fa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/18 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce3a0c6f49324d698bbcdf1d8cd87237"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/153 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4ba02d16ce25493697f61ca3025c20e4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/18 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "42208baa092640e2a79132f1bafca80d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize with Alignment\n",
    "\n",
    "# noinspection PyPackageRequirements \n",
    "from transformers import AutoTokenizer  # it is provided by adapter-transformers==3.0.1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dumitrescustefan/bert-base-romanian-cased-v1\")\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized = tokenizer(example[\"words\"], is_split_into_words=True, truncation=True, max_length=512)\n",
    "    word_ids = tokenized.word_ids()\n",
    "    aligned_labels = []\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            aligned_labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            aligned_labels.append(label2id[example[\"word_labels\"][word_idx]])\n",
    "        else:\n",
    "            aligned_labels.append(label2id[example[\"word_labels\"][word_idx]])  # or -100\n",
    "        previous_word_idx = word_idx\n",
    "    tokenized[\"labels\"] = aligned_labels\n",
    "    tokenizer.model_max_length = 512\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "def normalize_text(example):\n",
    "    example[\"summary\"] = unicodedata.normalize(\"NFC\", example[\"summary\"])\n",
    "    return example\n",
    "\n",
    "\n",
    "dataset = dataset.map(normalize_text)\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, remove_columns=dataset[\"train\"].column_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-04T12:10:30.005148200Z",
     "start_time": "2025-10-04T12:10:20.546307200Z"
    }
   },
   "id": "56ab4ec5d43e53d0"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Spre', 'uimire', '##a', 'microbi', '##știlor', ',', 'meciul', 'zilei', 'nu', 'vine', 'din', 'Europa', 'League', '.', 'După', 'ce', 'a', 'def', '##ilat', 'în', 'Champions', 'League', ',', 'a', 'făcut', 'un', 'rezultat', 'uluitor', 'în', 'campionatul', 'intern', '[SEP]']\n",
      "[-100, 0, 2, 2, 2, 2, 0, 2, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 2, 0, 1, 1, 0, 0, 0, 0, 2, 2, 0, 1, 1, -100]\n"
     ]
    }
   ],
   "source": [
    "# Inspect a sample\n",
    "sample = tokenized_dataset[\"train\"][0]\n",
    "print(tokenizer.convert_ids_to_tokens(sample[\"input_ids\"]))\n",
    "print(sample[\"labels\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-04T12:10:30.040125100Z",
     "start_time": "2025-10-04T12:10:29.986333Z"
    }
   },
   "id": "7796d5d539faeb72"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load Model\n",
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"dumitrescustefan/bert-base-romanian-cased-v1\",\n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c190ddfa8eb6e7"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Setup trainer\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"dumitrescustefan_token_output\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    max_steps=200,\n",
    "    logging_dir=\"logs\",\n",
    "    logging_steps=20,\n",
    "    save_steps=50,\n",
    "    do_eval=True,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "# class NoPinTrainer(Trainer):\n",
    "#     def get_train_dataloader(self):\n",
    "#         dataloader = super().get_train_dataloader()\n",
    "#         dataloader.pin_memory = False\n",
    "#         return dataloader\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-04T12:10:30.685653900Z",
     "start_time": "2025-10-04T12:10:30.537105800Z"
    }
   },
   "id": "57de3bf8c6d29e58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train and evaluate\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cc39572cfb5f047"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Torch script style\n",
    "\n",
    "# 1. Load the trained checkpoint\n",
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "ts_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"dumitrescustefan_token_output/checkpoint-200\",\n",
    "    num_labels=3,\n",
    "    id2label={0: 'O', 1: 'B-ENT', 2: 'B-KW'},\n",
    "    label2id={'O': 0, 'B-ENT': 1, 'B-KW': 2}\n",
    ")\n",
    "ts_model.eval()\n",
    "\n",
    "# 2. Create a valid example input\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dumitrescustefan_token_output/checkpoint-200\")\n",
    "\n",
    "text = \"Simona Halep a câștigat meciul de la Roland Garros.\"\n",
    "words = re.findall(r\"\\w+|\\S\", text)\n",
    "\n",
    "encoding = tokenizer(\n",
    "    words,\n",
    "    is_split_into_words=True,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "# 3. Trace and save the model\n",
    "import torch\n",
    "\n",
    "\n",
    "class Wrapper(torch.nn.Module):\n",
    "    def __init__(self, model_ts):\n",
    "        super().__init__()\n",
    "        self.model = model_ts\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "\n",
    "wrapped_model = Wrapper(ts_model)\n",
    "traced = torch.jit.trace(wrapped_model, (encoding[\"input_ids\"], encoding[\"attention_mask\"]))\n",
    "traced.save(\"model.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-08T09:34:46.050647700Z",
     "start_time": "2025-10-08T09:34:39.883768700Z"
    }
   },
   "id": "2f3bb4d3fc32a396"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
