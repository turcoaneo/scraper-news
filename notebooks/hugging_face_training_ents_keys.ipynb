{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "import os\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "from service.util.path_util import PROJECT_ROOT\n",
    "\n",
    "BASE_DIR = PROJECT_ROOT\n",
    "JSONL_PATH = os.path.join(BASE_DIR, \"storage\", \"hf\", \"gpt_20250930.jsonl\")\n",
    "\n",
    "dataset = Dataset.from_json(JSONL_PATH, encoding=\"utf-8\")\n",
    "dataset = dataset.train_test_split(test_size=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-09T10:17:57.926690600Z",
     "start_time": "2025-11-09T10:17:57.777577400Z"
    }
   },
   "id": "3997014fcc769914"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Define Label Mapping\n",
    "label2id = {\"O\": 0, \"B-ENT\": 1, \"B-KW\": 2}\n",
    "id2label = {v: k for k, v in label2id.items()}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-09T10:17:57.935095800Z",
     "start_time": "2025-11-09T10:17:57.929716100Z"
    }
   },
   "id": "a2ea117e90278500"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/153 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ede335095a374bc1a66167438b77cd1b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/18 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "de430b84210847c6a3e07940c0c979e1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Convert Each Example to Tokens + Labels\n",
    "import re\n",
    "def split_words(txt):\n",
    "    return re.findall(r\"\\w+|\\S\", txt)\n",
    "\n",
    "\n",
    "def label_words(example):\n",
    "    summary_words = split_words(example[\"summary\"])\n",
    "    labels = [\"O\"] * len(summary_words)\n",
    "\n",
    "    def mark_span(phrase, label_tag):\n",
    "        phrase_words = split_words(phrase)\n",
    "        for i in range(len(summary_words) - len(phrase_words) + 1):\n",
    "            if summary_words[i:i + len(phrase_words)] == phrase_words:\n",
    "                for j in range(len(phrase_words)):\n",
    "                    labels[i + j] = label_tag\n",
    "\n",
    "    for ent in example[\"entities\"]:\n",
    "        mark_span(ent, \"B-ENT\")\n",
    "    for kw in example[\"keywords\"]:\n",
    "        mark_span(kw, \"B-KW\")\n",
    "\n",
    "    return {\"words\": summary_words, \"word_labels\": labels}\n",
    "\n",
    "\n",
    "dataset = dataset.map(label_words)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-09T10:17:58.103253600Z",
     "start_time": "2025-11-09T10:17:57.936094300Z"
    }
   },
   "id": "b1712cb1f22f3cca"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "74c40a4db8c246bf85d162babf3c447d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\DEV\\scraper-news\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ovidi\\.cache\\huggingface\\hub\\models--dumitrescustefan--bert-base-romanian-cased-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f50881b9cdbb43a4967af1a0a65241ef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.txt: 0.00B [00:00, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2747d508b35d483e8fea6090fc450753"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/153 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7d56696660f148acb827629de88e6041"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/18 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5278bc1e04874aca8436b31c83aca746"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/153 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "98589a86674340c7bd79eb1bca773713"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/18 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ad5c3bc844d403188e09578118f0679"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize with Alignment\n",
    "\n",
    "# noinspection PyPackageRequirements \n",
    "from transformers import AutoTokenizer  # it is provided by adapter-transformers==3.0.1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dumitrescustefan/bert-base-romanian-cased-v1\")\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized = tokenizer(example[\"words\"], is_split_into_words=True, truncation=True, max_length=512)\n",
    "    word_ids = tokenized.word_ids()\n",
    "    aligned_labels = []\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            aligned_labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            aligned_labels.append(label2id[example[\"word_labels\"][word_idx]])\n",
    "        else:\n",
    "            aligned_labels.append(label2id[example[\"word_labels\"][word_idx]])  # or -100\n",
    "        previous_word_idx = word_idx\n",
    "    tokenized[\"labels\"] = aligned_labels\n",
    "    tokenizer.model_max_length = 512\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "def normalize_text(example):\n",
    "    example[\"summary\"] = unicodedata.normalize(\"NFC\", example[\"summary\"])\n",
    "    return example\n",
    "\n",
    "\n",
    "dataset = dataset.map(normalize_text)\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, remove_columns=dataset[\"train\"].column_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-09T10:18:04.595920700Z",
     "start_time": "2025-11-09T10:17:58.104864800Z"
    }
   },
   "id": "56ab4ec5d43e53d0"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Alin', 'Buzăr', '##in', 'scrie', ',', 'pe', 'GSP', '.', 'ro', ',', 'despre', 'decizia', '-', 'șoc', 'a', 'mijlocașul', '##ui', 'din', 'China', '[SEP]']\n",
      "[-100, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 2, 2, 2, 0, 0, 0, 0, 1, -100]\n"
     ]
    }
   ],
   "source": [
    "# Inspect a sample\n",
    "sample = tokenized_dataset[\"train\"][0]\n",
    "print(tokenizer.convert_ids_to_tokens(sample[\"input_ids\"]))\n",
    "print(sample[\"labels\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-09T10:18:04.607466600Z",
     "start_time": "2025-11-09T10:18:04.598936500Z"
    }
   },
   "id": "7796d5d539faeb72"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/500M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "082b517b7c85492bb7f338f402e900e2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dumitrescustefan/bert-base-romanian-cased-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load Model\n",
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"dumitrescustefan/bert-base-romanian-cased-v1\",\n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-09T10:19:42.976399500Z",
     "start_time": "2025-11-09T10:18:04.604457800Z"
    }
   },
   "id": "7c190ddfa8eb6e7"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Setup trainer\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"dumitrescustefan_token_output\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    max_steps=200,\n",
    "    logging_dir=\"logs\",\n",
    "    logging_steps=20,\n",
    "    save_steps=50,\n",
    "    do_eval=True,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "# class NoPinTrainer(Trainer):\n",
    "#     def get_train_dataloader(self):\n",
    "#         dataloader = super().get_train_dataloader()\n",
    "#         dataloader.pin_memory = False\n",
    "#         return dataloader\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-09T10:19:43.139839300Z",
     "start_time": "2025-11-09T10:19:42.977400Z"
    }
   },
   "id": "57de3bf8c6d29e58"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\DEV\\scraper-news\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/200 : < :, Epoch 0.05/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\DEV\\scraper-news\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "D:\\DEV\\scraper-news\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "D:\\DEV\\scraper-news\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "D:\\DEV\\scraper-news\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/3 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "{'eval_loss': 0.6812012195587158,\n 'eval_runtime': 0.5523,\n 'eval_samples_per_second': 32.589,\n 'eval_steps_per_second': 5.432,\n 'epoch': 10.0}"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and evaluate\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-09T10:22:45.482223600Z",
     "start_time": "2025-11-09T10:19:43.140840200Z"
    }
   },
   "id": "8cc39572cfb5f047"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
      "D:\\DEV\\scraper-news\\venv\\Lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:196: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  inverted_mask = torch.tensor(1.0, dtype=dtype) - expanded_mask\n"
     ]
    }
   ],
   "source": [
    "# Torch script style\n",
    "\n",
    "# 1. Load the trained checkpoint\n",
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "ts_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"dumitrescustefan_token_output/checkpoint-200\",\n",
    "    num_labels=3,\n",
    "    id2label={0: 'O', 1: 'B-ENT', 2: 'B-KW'},\n",
    "    label2id={'O': 0, 'B-ENT': 1, 'B-KW': 2}\n",
    ")\n",
    "ts_model.eval()\n",
    "\n",
    "# 2. Create a valid example input\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dumitrescustefan_token_output/checkpoint-200\")\n",
    "\n",
    "text = \"Simona Halep a câștigat meciul de la Roland Garros.\"\n",
    "words = re.findall(r\"\\w+|\\S\", text)\n",
    "\n",
    "encoding = tokenizer(\n",
    "    words,\n",
    "    is_split_into_words=True,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "# 3. Trace and save the model\n",
    "import torch\n",
    "\n",
    "\n",
    "class Wrapper(torch.nn.Module):\n",
    "    def __init__(self, model_ts):\n",
    "        super().__init__()\n",
    "        self.model = model_ts\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "\n",
    "wrapped_model = Wrapper(ts_model)\n",
    "traced = torch.jit.trace(wrapped_model, (encoding[\"input_ids\"], encoding[\"attention_mask\"]))\n",
    "traced.save(\"bert_model.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-09T10:22:50.214869500Z",
     "start_time": "2025-11-09T10:22:45.484220500Z"
    }
   },
   "id": "2f3bb4d3fc32a396"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
