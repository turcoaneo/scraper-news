{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-03T11:45:06.236647600Z",
     "start_time": "2025-11-03T11:45:06.198150100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported 358 duplets to D:\\WORKSPACE\\Python\\scraper-news\\storage\\hybrid_duplets.txt\n"
     ]
    }
   ],
   "source": [
    "# Convert duplets\n",
    "import os\n",
    "\n",
    "from service.util.path_util import PROJECT_ROOT\n",
    "from storage.duplets_dictionary import duplets, new_duplets\n",
    "\n",
    "BASE_DIR = PROJECT_ROOT\n",
    "HYBRID_PATH = os.path.join(BASE_DIR, \"storage\", \"hybrid_duplets.txt\")\n",
    "\n",
    "def export_duplets_for_t5(p_duplets, output_path=HYBRID_PATH):\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for inflected, base in p_duplets:\n",
    "            p_input_text = f\"normalize: {inflected}\"\n",
    "            p_target_text = base\n",
    "            f.write(f\"{p_input_text}\\t{p_target_text}\\n\")\n",
    "    print(f\"Exported {len(p_duplets)} duplets to {output_path}\")\n",
    "\n",
    "\n",
    "export_duplets_for_t5(duplets + new_duplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Load the File\n",
    "examples = []\n",
    "with open(HYBRID_PATH, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        input_text, target_text = line.strip().split(\"\\t\")\n",
    "        examples.append((input_text, target_text))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-03T11:45:06.375423Z",
     "start_time": "2025-11-03T11:45:06.243648500Z"
    }
   },
   "id": "66a341e1b56ce577"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Șucu → [37]\n",
      "Țiriac → [52]\n",
      "Ștefănești → [120]\n",
      "Cîrstea → [58, 90, 75]\n",
      "Mureșan → [24, 16]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize for T5\n",
    "\n",
    "# t5_model = \"google/mt5-small\"\n",
    "t5_model = \"t5-small\"\n",
    "\n",
    "from transformers import T5Tokenizer\n",
    "# Load Hugging Face tokenizer\n",
    "hf_tokenizer = T5Tokenizer.from_pretrained(t5_model, legacy=False)\n",
    "\n",
    "# Load SP tokenizer\n",
    "import sentencepiece as spm\n",
    "\n",
    "sp_tokenizer = spm.SentencePieceProcessor()\n",
    "sp_tokenizer.load(\"pre-declension.model\")\n",
    "\n",
    "# Test SP tokenization\n",
    "for word in [\"Șucu\", \"Țiriac\", \"Ștefănești\", \"Cîrstea\", \"Mureșan\"]:\n",
    "    print(f\"{word} → {sp_tokenizer.tokenize(word)}\")\n",
    "\n",
    "def hybrid_tokenize(text, p_sp_tokenizer, p_hf_tokenizer):\n",
    "    sp_tokens = p_sp_tokenizer.encode(text, out_type=str)\n",
    "    merged = \" \".join(sp_tokens)\n",
    "    return p_hf_tokenizer(merged, truncation=True, padding=\"max_length\", max_length=32, return_tensors=\"pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-03T11:45:14.930075700Z",
     "start_time": "2025-11-03T11:45:06.337426Z"
    }
   },
   "id": "80a4325a66f33deb"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": "Embedding(32191, 512)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train with T5ForConditionalGeneration\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Add known entities to tokenizer vocab\n",
    "seen = set()\n",
    "for _, second in duplets:\n",
    "    seen.add(second)\n",
    "\n",
    "# hf_tokenizer.add_tokens(list(seen))\n",
    "\n",
    "if \"mt5\" in t5_model:\n",
    "    from transformers import MT5ForConditionalGeneration\n",
    "    model = MT5ForConditionalGeneration.from_pretrained(t5_model)\n",
    "else:\n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    model = T5ForConditionalGeneration.from_pretrained(t5_model)\n",
    "\n",
    "model.resize_token_embeddings(len(hf_tokenizer))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-03T11:45:19.905981200Z",
     "start_time": "2025-11-03T11:45:14.917101900Z"
    }
   },
   "id": "1afa4a39d1d05e2f"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n",
      "Tokenizer test:\n",
      "Dan Șucu → ['▁Dan', '▁', 'Ș', 'u', 'cu']\n",
      "Șucu → ['▁', 'Ș', 'u', 'cu']\n",
      "Țiriac → ['▁', 'Ț', 'i', 'r', 'i', 'a', 'c']\n",
      "Șimon → ['▁', 'Ș', 'i', 'mon']\n",
      "Țăranu → ['▁', 'Ț', 'ă', 'ran', 'u']\n",
      "Sorana Cîrstea → ['▁So', 'ran', 'a', '▁C', 'î', 'r', 'stea']\n",
      "Soranei Cîrstea → ['▁So', 'ran', 'e', 'i', '▁C', 'î', 'r', 'stea']\n",
      "Iuliu Mureșan → ['▁I', 'ul', 'i', 'u', '▁Mur', 'e', 'ș', 'an']\n",
      "Total loss: 428.5199 - 224.499s\n",
      "\n",
      "Epoch 2/20\n",
      "Tokenizer test:\n",
      "Dan Șucu → ['▁Dan', '▁', 'Ș', 'u', 'cu']\n",
      "Șucu → ['▁', 'Ș', 'u', 'cu']\n",
      "Țiriac → ['▁', 'Ț', 'i', 'r', 'i', 'a', 'c']\n",
      "Șimon → ['▁', 'Ș', 'i', 'mon']\n",
      "Țăranu → ['▁', 'Ț', 'ă', 'ran', 'u']\n",
      "Sorana Cîrstea → ['▁So', 'ran', 'a', '▁C', 'î', 'r', 'stea']\n",
      "Soranei Cîrstea → ['▁So', 'ran', 'e', 'i', '▁C', 'î', 'r', 'stea']\n",
      "Iuliu Mureșan → ['▁I', 'ul', 'i', 'u', '▁Mur', 'e', 'ș', 'an']\n",
      "Total loss: 159.4229 - 214.568s\n",
      "\n",
      "Epoch 3/20\n",
      "Tokenizer test:\n",
      "Dan Șucu → ['▁Dan', '▁', 'Ș', 'u', 'cu']\n",
      "Șucu → ['▁', 'Ș', 'u', 'cu']\n",
      "Țiriac → ['▁', 'Ț', 'i', 'r', 'i', 'a', 'c']\n",
      "Șimon → ['▁', 'Ș', 'i', 'mon']\n",
      "Țăranu → ['▁', 'Ț', 'ă', 'ran', 'u']\n",
      "Sorana Cîrstea → ['▁So', 'ran', 'a', '▁C', 'î', 'r', 'stea']\n",
      "Soranei Cîrstea → ['▁So', 'ran', 'e', 'i', '▁C', 'î', 'r', 'stea']\n",
      "Iuliu Mureșan → ['▁I', 'ul', 'i', 'u', '▁Mur', 'e', 'ș', 'an']\n",
      "Total loss: 125.6509 - 213.557s\n",
      "\n",
      "Epoch 4/20\n",
      "Tokenizer test:\n",
      "Dan Șucu → ['▁Dan', '▁', 'Ș', 'u', 'cu']\n",
      "Șucu → ['▁', 'Ș', 'u', 'cu']\n",
      "Țiriac → ['▁', 'Ț', 'i', 'r', 'i', 'a', 'c']\n",
      "Șimon → ['▁', 'Ș', 'i', 'mon']\n",
      "Țăranu → ['▁', 'Ț', 'ă', 'ran', 'u']\n",
      "Sorana Cîrstea → ['▁So', 'ran', 'a', '▁C', 'î', 'r', 'stea']\n",
      "Soranei Cîrstea → ['▁So', 'ran', 'e', 'i', '▁C', 'î', 'r', 'stea']\n",
      "Iuliu Mureșan → ['▁I', 'ul', 'i', 'u', '▁Mur', 'e', 'ș', 'an']\n",
      "Total loss: 102.8314 - 214.765s\n",
      "\n",
      "Epoch 5/20\n",
      "Tokenizer test:\n",
      "Dan Șucu → ['▁Dan', '▁', 'Ș', 'u', 'cu']\n",
      "Șucu → ['▁', 'Ș', 'u', 'cu']\n",
      "Țiriac → ['▁', 'Ț', 'i', 'r', 'i', 'a', 'c']\n",
      "Șimon → ['▁', 'Ș', 'i', 'mon']\n",
      "Țăranu → ['▁', 'Ț', 'ă', 'ran', 'u']\n",
      "Sorana Cîrstea → ['▁So', 'ran', 'a', '▁C', 'î', 'r', 'stea']\n",
      "Soranei Cîrstea → ['▁So', 'ran', 'e', 'i', '▁C', 'î', 'r', 'stea']\n",
      "Iuliu Mureșan → ['▁I', 'ul', 'i', 'u', '▁Mur', 'e', 'ș', 'an']\n",
      "Total loss: 86.8652 - 219.959s\n",
      "\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 19\u001B[39m\n\u001B[32m     16\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m+\u001B[38;5;250m \u001B[39m\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     18\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m input_batch, label_batch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(inputs, labels):\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m     output = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43minput_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlabel_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     20\u001B[39m     loss = output.loss\n\u001B[32m     21\u001B[39m     loss.backward()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\WORKSPACE\\Python\\scraper-news\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\WORKSPACE\\Python\\scraper-news\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\WORKSPACE\\Python\\scraper-news\\venv\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1727\u001B[39m, in \u001B[36mT5ForConditionalGeneration.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001B[39m\n\u001B[32m   1724\u001B[39m \u001B[38;5;66;03m# Encode if needed (training, first prediction pass)\u001B[39;00m\n\u001B[32m   1725\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m encoder_outputs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1726\u001B[39m     \u001B[38;5;66;03m# Convert encoder inputs in embeddings if needed\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1727\u001B[39m     encoder_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1728\u001B[39m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1729\u001B[39m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1730\u001B[39m \u001B[43m        \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1731\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1732\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1733\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1734\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1735\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1736\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m return_dict \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(encoder_outputs, BaseModelOutput):\n\u001B[32m   1737\u001B[39m     encoder_outputs = BaseModelOutput(\n\u001B[32m   1738\u001B[39m         last_hidden_state=encoder_outputs[\u001B[32m0\u001B[39m],\n\u001B[32m   1739\u001B[39m         hidden_states=encoder_outputs[\u001B[32m1\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(encoder_outputs) > \u001B[32m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1740\u001B[39m         attentions=encoder_outputs[\u001B[32m2\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(encoder_outputs) > \u001B[32m2\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1741\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\WORKSPACE\\Python\\scraper-news\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\WORKSPACE\\Python\\scraper-news\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\WORKSPACE\\Python\\scraper-news\\venv\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1100\u001B[39m, in \u001B[36mT5Stack.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001B[39m\n\u001B[32m   1097\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m output_hidden_states:\n\u001B[32m   1098\u001B[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001B[32m-> \u001B[39m\u001B[32m1100\u001B[39m layer_outputs = \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1101\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1102\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcausal_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1103\u001B[39m \u001B[43m    \u001B[49m\u001B[43mposition_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1104\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1105\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1106\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_decoder_position_bias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001B[39;49;00m\n\u001B[32m   1107\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1108\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcross_attn_layer_head_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcross_attn_layer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1109\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1110\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1111\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1112\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1113\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1114\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1116\u001B[39m hidden_states = layer_outputs[\u001B[32m0\u001B[39m]\n\u001B[32m   1118\u001B[39m \u001B[38;5;66;03m# We share the position biases between the layers - the first layer store them\u001B[39;00m\n\u001B[32m   1119\u001B[39m \u001B[38;5;66;03m# layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),\u001B[39;00m\n\u001B[32m   1120\u001B[39m \u001B[38;5;66;03m# (cross-attention position bias), (cross-attention weights)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\WORKSPACE\\Python\\scraper-news\\venv\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001B[39m, in \u001B[36mGradientCheckpointingLayer.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m     91\u001B[39m         logger.warning_once(message)\n\u001B[32m     93\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._gradient_checkpointing_func(partial(\u001B[38;5;28msuper\u001B[39m().\u001B[34m__call__\u001B[39m, **kwargs), *args)\n\u001B[32m---> \u001B[39m\u001B[32m94\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\WORKSPACE\\Python\\scraper-news\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\WORKSPACE\\Python\\scraper-news\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\WORKSPACE\\Python\\scraper-news\\venv\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001B[39m, in \u001B[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    168\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m minimum_action \u001B[38;5;129;01min\u001B[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():\n\u001B[32m    169\u001B[39m     \u001B[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001B[39;00m\n\u001B[32m    170\u001B[39m     warnings.warn(message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel=\u001B[32m2\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m172\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\WORKSPACE\\Python\\scraper-news\\venv\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:737\u001B[39m, in \u001B[36mT5Block.forward\u001B[39m\u001B[34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_values, use_cache, output_attentions, return_dict, cache_position)\u001B[39m\n\u001B[32m    734\u001B[39m     attention_outputs = attention_outputs + cross_attention_outputs[\u001B[32m1\u001B[39m:]\n\u001B[32m    736\u001B[39m \u001B[38;5;66;03m# Apply Feed Forward layer\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m737\u001B[39m hidden_states = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlayer\u001B[49m\u001B[43m[\u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    739\u001B[39m \u001B[38;5;66;03m# clamp inf values to enable fp16 training\u001B[39;00m\n\u001B[32m    740\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m hidden_states.dtype == torch.float16:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\WORKSPACE\\Python\\scraper-news\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\WORKSPACE\\Python\\scraper-news\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\WORKSPACE\\Python\\scraper-news\\venv\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:343\u001B[39m, in \u001B[36mT5LayerFF.forward\u001B[39m\u001B[34m(self, hidden_states)\u001B[39m\n\u001B[32m    341\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, hidden_states):\n\u001B[32m    342\u001B[39m     forwarded_states = \u001B[38;5;28mself\u001B[39m.layer_norm(hidden_states)\n\u001B[32m--> \u001B[39m\u001B[32m343\u001B[39m     forwarded_states = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mDenseReluDense\u001B[49m\u001B[43m(\u001B[49m\u001B[43mforwarded_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    344\u001B[39m     hidden_states = hidden_states + \u001B[38;5;28mself\u001B[39m.dropout(forwarded_states)\n\u001B[32m    345\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m hidden_states\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\WORKSPACE\\Python\\scraper-news\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\WORKSPACE\\Python\\scraper-news\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\WORKSPACE\\Python\\scraper-news\\venv\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:290\u001B[39m, in \u001B[36mT5DenseActDense.forward\u001B[39m\u001B[34m(self, hidden_states)\u001B[39m\n\u001B[32m    288\u001B[39m hidden_states = \u001B[38;5;28mself\u001B[39m.wi(hidden_states)\n\u001B[32m    289\u001B[39m hidden_states = \u001B[38;5;28mself\u001B[39m.act(hidden_states)\n\u001B[32m--> \u001B[39m\u001B[32m290\u001B[39m hidden_states = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdropout\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    291\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    292\u001B[39m     \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m.wo.weight, torch.Tensor)\n\u001B[32m    293\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m hidden_states.dtype != \u001B[38;5;28mself\u001B[39m.wo.weight.dtype\n\u001B[32m    294\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.wo.weight.dtype != torch.int8\n\u001B[32m    295\u001B[39m ):\n\u001B[32m    296\u001B[39m     hidden_states = hidden_states.to(\u001B[38;5;28mself\u001B[39m.wo.weight.dtype)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\WORKSPACE\\Python\\scraper-news\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\WORKSPACE\\Python\\scraper-news\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\WORKSPACE\\Python\\scraper-news\\venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:70\u001B[39m, in \u001B[36mDropout.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m     69\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m---> \u001B[39m\u001B[32m70\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdropout\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43minplace\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\WORKSPACE\\Python\\scraper-news\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1422\u001B[39m, in \u001B[36mdropout\u001B[39m\u001B[34m(input, p, training, inplace)\u001B[39m\n\u001B[32m   1419\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m p < \u001B[32m0.0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m p > \u001B[32m1.0\u001B[39m:\n\u001B[32m   1420\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mdropout probability has to be between 0 and 1, but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mp\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m   1421\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[32m-> \u001B[39m\u001B[32m1422\u001B[39m     _VF.dropout_(\u001B[38;5;28minput\u001B[39m, p, training) \u001B[38;5;28;01mif\u001B[39;00m inplace \u001B[38;5;28;01melse\u001B[39;00m \u001B[43m_VF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdropout\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1423\u001B[39m )\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "source": [
    "# Tokenize inputs and labels\n",
    "inputs = [hybrid_tokenize(x, sp_tokenizer, hf_tokenizer) for x, _ in examples]\n",
    "labels = [hybrid_tokenize(y, sp_tokenizer, hf_tokenizer)[\"input_ids\"] for _, y in examples]\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "model.train()\n",
    "\n",
    "num_epochs = 20\n",
    "limit = num_epochs / 5\n",
    "min_loss = float(\"inf\")\n",
    "best_hybrid_model_path = \"hybrid_declension_model_best\"\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    for input_batch, label_batch in zip(inputs, labels):\n",
    "        output = model(**input_batch, labels=label_batch)\n",
    "        loss = output.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    duration = time.time() - start\n",
    "    print(\"Tokenizer test:\")\n",
    "    for word in [\"Dan Șucu\",\"Șucu\", \"Țiriac\", \"Șimon\", \"Țăranu\", \"Sorana Cîrstea\", \"Soranei Cîrstea\", \"Iuliu Mureșan\"]:\n",
    "        print(f\"{word} → {hf_tokenizer.tokenize(word)}\")\n",
    "    print(f\"Total loss: {total_loss:.4f} - {duration:.3f}s\")\n",
    "\n",
    "    # Save best model only in the last 5 epochs\n",
    "    if epoch >= num_epochs - limit and total_loss < min_loss:\n",
    "        min_loss = total_loss\n",
    "        model.save_pretrained(best_hybrid_model_path)\n",
    "        hf_tokenizer.save_pretrained(best_hybrid_model_path)\n",
    "        print(f\"✅ Saved best model at epoch {epoch + 1} with loss {total_loss:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-03T12:04:52.613970700Z",
     "start_time": "2025-11-03T11:45:19.901990600Z"
    }
   },
   "id": "d4aad0cfbe49c6bc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save and tokenize\n",
    "hybrid_model_path = \"hybrid_declension_model\"\n",
    "model.save_pretrained(hybrid_model_path)\n",
    "hf_tokenizer.save_pretrained(hybrid_model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-11-03T12:04:52.636956300Z",
     "start_time": "2025-11-03T12:04:52.614962200Z"
    }
   },
   "id": "24fd263511309fcd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load model\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "best_hybrid_model_path = \"hybrid_declension_model_best\"\n",
    "hybrid_model_path = \"hybrid_declension_model\"\n",
    "hf_tokenizer = T5Tokenizer.from_pretrained(hybrid_model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(hybrid_model_path)\n",
    "\n",
    "best_tokenizer = T5Tokenizer.from_pretrained(best_hybrid_model_path)\n",
    "best_model = T5ForConditionalGeneration.from_pretrained(best_hybrid_model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-11-03T12:04:52.617957200Z"
    }
   },
   "id": "890b9d78ad81f06"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test cases\n",
    "from storage.duplets_dictionary import test_cases\n",
    "from service.util.declension_util import DeclensionUtil\n",
    "\n",
    "model.eval()\n",
    "for input_text, expected_output in test_cases:\n",
    "    scripted_model_output = DeclensionUtil.normalize(input_text, (hf_tokenizer, model))\n",
    "    best_scripted_model_output = DeclensionUtil.normalize(input_text, (best_tokenizer, best_model))\n",
    "    if scripted_model_output != expected_output:\n",
    "        print(f\"❌ {input_text} → {best_scripted_model_output} - {scripted_model_output} (expected: {expected_output})\")\n",
    "    else:\n",
    "        print(f\"✅ {input_text} →  {best_scripted_model_output} - {scripted_model_output}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-11-03T12:04:52.620968300Z"
    }
   },
   "id": "787ec472b565b165"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
