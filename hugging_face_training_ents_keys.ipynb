{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset=Dataset.from_json(\"storage/hf/gpt_20250930.jsonl\", encoding=\"utf-8\")\n",
    "dataset = dataset.train_test_split(test_size=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-03T09:59:56.372620400Z",
     "start_time": "2025-10-03T09:59:51.149376700Z"
    }
   },
   "id": "3997014fcc769914"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Define Label Mapping\n",
    "label2id = {\"O\": 0, \"B-ENT\": 1, \"B-KW\": 2}\n",
    "id2label = {v: k for k, v in label2id.items()}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-03T09:59:58.599949800Z",
     "start_time": "2025-10-03T09:59:58.586528500Z"
    }
   },
   "id": "a2ea117e90278500"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/153 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4ab5129b5d274ef6b7ea72852e05fc0e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/18 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "27569611384b4572aea40c64098c2183"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Convert Each Example to Tokens + Labels\n",
    "import re\n",
    "\n",
    "def split_words(text):\n",
    "    return re.findall(r\"\\w+|\\S\", text)\n",
    "\n",
    "def label_words(example):\n",
    "    words = split_words(example[\"summary\"])\n",
    "    labels = [\"O\"] * len(words)\n",
    "\n",
    "    def mark_span(phrase, label_tag):\n",
    "        phrase_words = split_words(phrase)\n",
    "        for i in range(len(words) - len(phrase_words) + 1):\n",
    "            if words[i:i+len(phrase_words)] == phrase_words:\n",
    "                for j in range(len(phrase_words)):\n",
    "                    labels[i + j] = label_tag\n",
    "\n",
    "    for ent in example[\"entities\"]:\n",
    "        mark_span(ent, \"B-ENT\")\n",
    "    for kw in example[\"keywords\"]:\n",
    "        mark_span(kw, \"B-KW\")\n",
    "\n",
    "    return {\"words\": words, \"word_labels\": labels}\n",
    "\n",
    "dataset = dataset.map(label_words)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-03T10:00:01.486979200Z",
     "start_time": "2025-10-03T10:00:01.142928800Z"
    }
   },
   "id": "b1712cb1f22f3cca"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "45bee567136642ed86e1c4c7f21f757c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\WORKSPACE\\Python\\scraper-news\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ovidi\\.cache\\huggingface\\hub\\models--dumitrescustefan--bert-base-romanian-cased-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9714ec7e44144a7bb69cce041c899aad"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.txt: 0.00B [00:00, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "07f4e7e571fd45bb8a0f2fa2cb12d807"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/153 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "13e8a6f494ea41f79c1f03f833bc1e8e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/18 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "957cf132d3284a69a5f00e14976f49d2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/153 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "00ed7cca46ed40b596af4bdd77f200a7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/18 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dc8e5e2c051445d386d424b16f92f809"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize with Alignment\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dumitrescustefan/bert-base-romanian-cased-v1\")\n",
    "\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized = tokenizer(example[\"words\"], is_split_into_words=True, truncation=True)\n",
    "    word_ids = tokenized.word_ids()\n",
    "    aligned_labels = []\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            aligned_labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            aligned_labels.append(label2id[example[\"word_labels\"][word_idx]])\n",
    "        else:\n",
    "            aligned_labels.append(label2id[example[\"word_labels\"][word_idx]])  # or -100\n",
    "        previous_word_idx = word_idx\n",
    "    tokenized[\"labels\"] = aligned_labels\n",
    "    return tokenized\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "def normalize_text(example):\n",
    "    example[\"summary\"] = unicodedata.normalize(\"NFC\", example[\"summary\"])\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(normalize_text)\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, remove_columns=dataset[\"train\"].column_names)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-03T10:00:18.258056700Z",
     "start_time": "2025-10-03T10:00:04.581198200Z"
    }
   },
   "id": "56ab4ec5d43e53d0"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'FCSB', 'și', 'Universitatea', 'Craiova', 'vor', 'lupta', 'în', 'acest', 'sezon', 'european', 'pentru', 'o', 'ascensiune', 'a', 'Super', '##Li', '##gii', 'în', 'clasamentul', 'coefic', '##ienților', 'UEFA', '.', 'Locul', '20', 'ar', 'fi', 'varianta', 'cea', 'mai', 'optimistă', '.', '[SEP]']\n",
      "[-100, 1, 0, 1, 1, 0, 2, 0, 0, 2, 2, 0, 0, 2, 0, 1, 1, 1, 0, 2, 2, 2, 1, 0, 1, 1, 0, 0, 2, 2, 2, 2, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "# Inspect a sample\n",
    "sample = tokenized_dataset[\"train\"][0]\n",
    "print(tokenizer.convert_ids_to_tokens(sample[\"input_ids\"]))\n",
    "print(sample[\"labels\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-03T10:00:28.993110300Z",
     "start_time": "2025-10-03T10:00:28.965116400Z"
    }
   },
   "id": "7796d5d539faeb72"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/500M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0599f9bd3cd04bcb87b714cd39089967"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dumitrescustefan/bert-base-romanian-cased-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load Model\n",
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"dumitrescustefan/bert-base-romanian-cased-v1\",\n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-03T10:03:14.859821Z",
     "start_time": "2025-10-03T10:02:02.064891700Z"
    }
   },
   "id": "7c190ddfa8eb6e7"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Setup trainer\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"roberta_token_output\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    max_steps=200,\n",
    "    logging_dir=\"logs\",\n",
    "    logging_steps=20,\n",
    "    save_steps=50,\n",
    "    do_eval=True,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "class NoPinTrainer(Trainer):\n",
    "    def get_train_dataloader(self):\n",
    "        dataloader = super().get_train_dataloader()\n",
    "        dataloader.pin_memory = False\n",
    "        return dataloader\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-03T10:03:51.303985200Z",
     "start_time": "2025-10-03T10:03:51.147518800Z"
    }
   },
   "id": "57de3bf8c6d29e58"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\WORKSPACE\\Python\\scraper-news\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/200 : < :, Epoch 0.05/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\WORKSPACE\\Python\\scraper-news\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "D:\\WORKSPACE\\Python\\scraper-news\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "D:\\WORKSPACE\\Python\\scraper-news\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "D:\\WORKSPACE\\Python\\scraper-news\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/3 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "{'eval_loss': 0.7228062748908997,\n 'eval_runtime': 2.0287,\n 'eval_samples_per_second': 8.873,\n 'eval_steps_per_second': 1.479,\n 'epoch': 10.0}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and evaluate\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-03T10:14:22.855964900Z",
     "start_time": "2025-10-03T10:04:01.047149700Z"
    }
   },
   "id": "8cc39572cfb5f047"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# trainer.train(resume_from_checkpoint=\"roberta_token_output/checkpoint-50\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-10-02T14:35:53.591420300Z"
    }
   },
   "id": "faaa77dcbbc38ec1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
